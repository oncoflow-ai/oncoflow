{
    "nbformat": 4,
    "nbformat_minor": 5,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4",
            "name": "04_sam3_exploration_COLAB.ipynb"
        },
        "accelerator": "GPU",
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.12"
        }
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 04 â€“ SAM3 / SAM2 Exploration (Google Colab GPU Version)\n",
                "\n",
                "> **OncoFlow Project** â€“ Brain Tumor Segmentation with SAM3/SAM2\n",
                "\n",
                "**Goal:** Explore Meta SAM3 (and SAM2 fallback) for brain tumor segmentation on P01 BraTS data.\n",
                "\n",
                "---\n",
                "\n",
                "## âš™ï¸ Setup Requirements\n",
                "**Before running:** Go to `Runtime â†’ Change runtime type â†’ T4 GPU` (or A100 if Pro)\n",
                "\n",
                "**Modes tested:**\n",
                "1. **Point-prompted** â€“ click on tumor centroid (from GT centroid)\n",
                "2. **Box-prompted** â€“ bounding box of GT tumor region\n",
                "3. **Automatic** â€“ no user prompt, model generates proposals\n",
                "\n",
                "**Models compared:**\n",
                "- SAM3 (`facebook/sam3`) â€” released Nov 2025 *(requires HuggingFace access request)*\n",
                "- SAM2 (`facebook/sam2-hiera-large`) â€” fallback if SAM3 not available\n",
                "- MedSAM2 (`bowang-lab/MedSAM2`) â€” medical-adapted variant\n",
                "\n",
                "---\n",
                "\n",
                "**ğŸ“‚ Data source:** P01 BraTS NIfTI files + tumor segmentation masks\n",
                "\n",
                "You have **two options** for data:\n",
                "- **Option A:** Upload from Google Drive (recommended for real data)\n",
                "- **Option B:** Generate synthetic MRI phantom data (for pipeline testing)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ğŸ“¦ Cell 1: Install Dependencies\n",
                "\n",
                "This installs SAM2 (stable, widely available) and attempts SAM3 (requires HuggingFace access).\n",
                "\n",
                "**Expected time:** ~3-5 minutes on first run."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# â”€â”€ Cell 1: Install all dependencies â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "import subprocess, sys\n",
                "\n",
                "def run(cmd):\n",
                "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
                "    if result.returncode != 0:\n",
                "        print(f'STDERR: {result.stderr[-500:]}')\n",
                "    return result.returncode == 0\n",
                "\n",
                "print('ğŸ“¦ Installing base scientific packages...')\n",
                "run('pip install -q nibabel pydicom SimpleITK scipy seaborn pandas matplotlib Pillow')\n",
                "\n",
                "print('ğŸ“¦ Installing SAM2 from GitHub (stable)...')\n",
                "run('pip install -q git+https://github.com/facebookresearch/sam2.git')\n",
                "\n",
                "print('ğŸ“¦ Attempting SAM3 install from GitHub...')\n",
                "sam3_ok = run('pip install -q git+https://github.com/facebookresearch/sam3.git')\n",
                "if sam3_ok:\n",
                "    print('âœ… SAM3 package installed (still need HuggingFace model access)')\n",
                "else:\n",
                "    print('âš ï¸  SAM3 pip install failed â€“ will fall back to SAM2')\n",
                "\n",
                "print('ğŸ“¦ Installing HuggingFace hub...')\n",
                "run('pip install -q huggingface_hub')\n",
                "\n",
                "print('\\nâœ… Installation complete!')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ğŸ”‘ Cell 2: HuggingFace Authentication\n",
                "\n",
                "Required to download SAM3/SAM2 model weights.\n",
                "\n",
                "**Steps:**\n",
                "1. Go to [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
                "2. Create a **Read** token\n",
                "3. Paste it below\n",
                "4. For SAM3 specifically, also request access at [huggingface.co/facebook/sam3](https://huggingface.co/facebook/sam3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# â”€â”€ Cell 2: HuggingFace Authentication â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "from huggingface_hub import login\n",
                "import os\n",
                "\n",
                "# Option 1: Paste your token directly (replace the string below)\n",
                "HF_TOKEN = ''  # â† paste your HF token here, e.g. 'hf_xxxxxxxxxxxx'\n",
                "\n",
                "# Option 2: Use Colab Secrets (more secure - add via the ğŸ”‘ icon in the left sidebar)\n",
                "if not HF_TOKEN:\n",
                "    try:\n",
                "        from google.colab import userdata\n",
                "        HF_TOKEN = userdata.get('HF_TOKEN')\n",
                "        print('âœ… Using token from Colab Secrets')\n",
                "    except Exception:\n",
                "        print('âš ï¸  No token found in Colab Secrets')\n",
                "\n",
                "if HF_TOKEN:\n",
                "    login(token=HF_TOKEN, add_to_git_credential=False)\n",
                "    print('âœ… Logged in to HuggingFace')\n",
                "else:\n",
                "    print('âš ï¸  No HF token provided. Model download may fail for gated models.')\n",
                "    print('   â†’ For SAM2, no token needed.')\n",
                "    print('   â†’ For SAM3, request access at https://huggingface.co/facebook/sam3')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ğŸ–¥ï¸ Cell 3: Environment & GPU Check"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# â”€â”€ Cell 3: Environment setup & GPU verification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "import sys, os, time\n",
                "from pathlib import Path\n",
                "import numpy as np\n",
                "import nibabel as nib\n",
                "import matplotlib.pyplot as plt\n",
                "import matplotlib.gridspec as gridspec\n",
                "import matplotlib.colors as mcolors\n",
                "import pandas as pd\n",
                "import seaborn as sns\n",
                "from scipy.ndimage import binary_erosion\n",
                "\n",
                "import torch\n",
                "\n",
                "# Check GPU\n",
                "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "print(f'Device: {DEVICE}')\n",
                "\n",
                "if DEVICE == 'cuda':\n",
                "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
                "    print(f'VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\n",
                "    print(f'CUDA version: {torch.version.cuda}')\n",
                "else:\n",
                "    print('âš ï¸  No CUDA GPU found! Go to Runtime â†’ Change runtime type â†’ GPU')\n",
                "    print('    SAM3/SAM2 inference will be VERY slow on CPU.')\n",
                "\n",
                "# Output directories\n",
                "BASE_DIR = Path('/content')\n",
                "OUT_DIR  = BASE_DIR / 'outputs' / '04_sam'\n",
                "DATA_DIR = BASE_DIR / 'data'\n",
                "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
                "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "print(f'\\nOutput dir: {OUT_DIR}')\n",
                "print(f'Data dir:   {DATA_DIR}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ğŸ“ Cell 4: Data Setup\n",
                "\n",
                "Choose **Option A** (Google Drive) or **Option B** (synthetic phantom data)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# â”€â”€ Cell 4A: Mount Google Drive (if using real P01 data) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "USE_DRIVE = True   # â† set to False to use synthetic phantom data instead\n",
                "\n",
                "if USE_DRIVE:\n",
                "    from google.colab import drive\n",
                "    drive.mount('/content/drive')\n",
                "\n",
                "    # â”€â”€ UPDATE THESE PATHS to match your Drive layout â”€â”€\n",
                "    BRATS_DIR = Path('/content/drive/MyDrive/OncoFlow/data/P01/BraTS')\n",
                "    MASK_DIR  = Path('/content/drive/MyDrive/OncoFlow/data/P01/tumor segmentation')\n",
                "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "\n",
                "    if BRATS_DIR.exists():\n",
                "        print(f'âœ… BraTS dir found: {BRATS_DIR}')\n",
                "    else:\n",
                "        print(f'âš ï¸  BraTS dir NOT found at: {BRATS_DIR}')\n",
                "        print('   â†’ Update the path above to match your Drive layout.')\n",
                "        print('   â†’ Or set USE_DRIVE = False to use synthetic data.')\n",
                "else:\n",
                "    print('â„¹ï¸  Skipping Drive mount â€“ will generate synthetic phantom data in Cell 4B.')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# â”€â”€ Cell 4B: Generate synthetic MRI phantom data (if no Drive) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "# This creates realistic-looking synthetic brain MRI volumes with tumor masks\n",
                "# so you can test the full pipeline without real patient data.\n",
                "\n",
                "def make_synthetic_brain_volume(shape=(128, 128, 64), seed=42):\n",
                "    \"\"\"Generate a synthetic brain MRI volume with a fake tumor.\"\"\"\n",
                "    rng = np.random.default_rng(seed)\n",
                "    vol = np.zeros(shape, dtype=np.float32)\n",
                "\n",
                "    # Brain ellipsoid\n",
                "    cx, cy, cz = shape[0]//2, shape[1]//2, shape[2]//2\n",
                "    rx, ry, rz = shape[0]//2 - 10, shape[1]//2 - 10, shape[2]//2 - 8\n",
                "    for z in range(shape[2]):\n",
                "        for y in range(shape[1]):\n",
                "            for x in range(shape[0]):\n",
                "                if ((x-cx)/rx)**2 + ((y-cy)/ry)**2 + ((z-cz)/rz)**2 <= 1:\n",
                "                    vol[x, y, z] = 0.6 + 0.4 * rng.random()\n",
                "\n",
                "    # Tumor region (bright sphere)\n",
                "    tx, ty, tz = cx + 20, cy + 10, cz + 5\n",
                "    tr = 12\n",
                "    mask = np.zeros(shape, dtype=np.float32)\n",
                "    for z in range(max(0, tz-tr), min(shape[2], tz+tr)):\n",
                "        for y in range(max(0, ty-tr), min(shape[1], ty+tr)):\n",
                "            for x in range(max(0, tx-tr), min(shape[0], tx+tr)):\n",
                "                if (x-tx)**2 + (y-ty)**2 + (z-tz)**2 <= tr**2:\n",
                "                    vol[x, y, z] = min(1.0, vol[x,y,z] + 0.5 + 0.3*rng.random())\n",
                "                    mask[x, y, z] = 1.0\n",
                "\n",
                "    # Add noise\n",
                "    vol += rng.normal(0, 0.03, shape).astype(np.float32)\n",
                "    vol = np.clip(vol, 0, 1)\n",
                "    return vol, mask\n",
                "\n",
                "if not USE_DRIVE:\n",
                "    print('ğŸ§  Generating synthetic brain MRI phantom data...')\n",
                "    TIMEPOINTS = ['baseline', 'fu1']\n",
                "    brats_paths = {}\n",
                "    mask_paths = {}\n",
                "    affine = np.eye(4)  # 1mm isotropic\n",
                "    affine[0,0] = affine[1,1] = affine[2,2] = 1.0\n",
                "\n",
                "    for i, tp in enumerate(TIMEPOINTS):\n",
                "        vol, mask = make_synthetic_brain_volume(shape=(128, 128, 64), seed=i*100)\n",
                "\n",
                "        # Save as NIfTI\n",
                "        tp_dir = DATA_DIR / tp\n",
                "        tp_dir.mkdir(exist_ok=True)\n",
                "        mask_dir = DATA_DIR / 'masks'\n",
                "        mask_dir.mkdir(exist_ok=True)\n",
                "\n",
                "        t1c_path = tp_dir / 't1c.nii.gz'\n",
                "        mask_path = mask_dir / f'P01_tumor_mask_{tp}.nii.gz'\n",
                "\n",
                "        nib.save(nib.Nifti1Image(vol, affine), str(t1c_path))\n",
                "        nib.save(nib.Nifti1Image(mask, affine), str(mask_path))\n",
                "\n",
                "        brats_paths[tp] = {'t1c': str(t1c_path)}\n",
                "        mask_paths[tp] = str(mask_path)\n",
                "\n",
                "        print(f'  âœ… {tp}: vol={vol.shape}, tumor_voxels={(mask>0).sum()}')\n",
                "\n",
                "    BRATS_DIR = DATA_DIR\n",
                "    MASK_DIR  = DATA_DIR / 'masks'\n",
                "    print('\\nâœ… Synthetic data ready!')\n",
                "    print('   Note: Dice scores with synthetic data are meaningless for model evaluation.')\n",
                "    print('   Use this mode only to test that the pipeline runs end-to-end.')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ğŸ› ï¸ Cell 5: Inline Utility Functions\n",
                "\n",
                "These are self-contained versions of the `utils/` helpers from the local repo, so this notebook is fully standalone in Colab."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# â”€â”€ Cell 5: Inline utility functions (standalone â€“ no local utils/ needed) â”€â”€â”€\n",
                "\n",
                "# â”€â”€ NIfTI helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "def load_nifti(path):\n",
                "    \"\"\"Load a NIfTI file. Returns (data_array, affine, header).\"\"\"\n",
                "    img = nib.load(str(path))\n",
                "    return img.get_fdata(dtype=np.float32), img.affine, img.header\n",
                "\n",
                "def save_nifti(arr, affine, out_path):\n",
                "    \"\"\"Save numpy array as NIfTI.\"\"\"\n",
                "    Path(out_path).parent.mkdir(parents=True, exist_ok=True)\n",
                "    nib.save(nib.Nifti1Image(arr, affine), str(out_path))\n",
                "\n",
                "# â”€â”€ Path helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "TIMEPOINTS_ORDER = ['baseline', 'fu1', 'fu2', 'fu3', 'fu4']\n",
                "MODALITIES = ['t1', 't1c', 't2', 'fla']\n",
                "\n",
                "def get_p01_brats_paths(data_root):\n",
                "    root = Path(data_root)\n",
                "    result = {}\n",
                "    for tp in TIMEPOINTS_ORDER:\n",
                "        tp_dir = root / tp\n",
                "        if tp_dir.exists():\n",
                "            result[tp] = {\n",
                "                mod: str(tp_dir / f'{mod}.nii.gz')\n",
                "                for mod in MODALITIES\n",
                "                if (tp_dir / f'{mod}.nii.gz').exists()\n",
                "            }\n",
                "    return result\n",
                "\n",
                "def get_p01_mask_paths(data_root):\n",
                "    root = Path(data_root)\n",
                "    result = {}\n",
                "    for tp in TIMEPOINTS_ORDER:\n",
                "        fpath = root / f'P01_tumor_mask_{tp}.nii.gz'\n",
                "        if fpath.exists():\n",
                "            result[tp] = str(fpath)\n",
                "    return result\n",
                "\n",
                "# â”€â”€ Metrics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "def dice_coefficient(pred, gt, smooth=1e-6):\n",
                "    pred = (pred > 0.5).astype(bool)\n",
                "    gt   = (gt   > 0.5).astype(bool)\n",
                "    intersection = (pred & gt).sum()\n",
                "    return float(2.0 * intersection + smooth) / float(pred.sum() + gt.sum() + smooth)\n",
                "\n",
                "def iou_score(pred, gt, smooth=1e-6):\n",
                "    pred = (pred > 0.5).astype(bool)\n",
                "    gt   = (gt   > 0.5).astype(bool)\n",
                "    intersection = (pred & gt).sum()\n",
                "    union = (pred | gt).sum()\n",
                "    return float(intersection + smooth) / float(union + smooth)\n",
                "\n",
                "def compute_volume_cm3(mask, spacing_mm):\n",
                "    voxel_vol_mm3 = float(np.prod(spacing_mm))\n",
                "    n_voxels = int((mask > 0.5).sum())\n",
                "    return (n_voxels * voxel_vol_mm3) / 1000.0\n",
                "\n",
                "class Timer:\n",
                "    def __init__(self, label=''):\n",
                "        self.label = label\n",
                "        self.elapsed = 0.0\n",
                "    def __enter__(self):\n",
                "        self._start = time.perf_counter()\n",
                "        return self\n",
                "    def __exit__(self, *args):\n",
                "        self.elapsed = time.perf_counter() - self._start\n",
                "        if self.label:\n",
                "            print(f'[Timer] {self.label}: {self.elapsed:.3f}s')\n",
                "\n",
                "class BenchmarkTracker:\n",
                "    def __init__(self):\n",
                "        self._rows = []\n",
                "    def add(self, model, timepoint, pred, gt, spacing, inference_s=0.0, extra=None):\n",
                "        row = {\n",
                "            'model': model, 'timepoint': timepoint,\n",
                "            'dice':  round(dice_coefficient(pred, gt), 4),\n",
                "            'iou':   round(iou_score(pred, gt), 4),\n",
                "            'volume_pred_cm3': round(compute_volume_cm3(pred, spacing), 4),\n",
                "            'volume_gt_cm3':   round(compute_volume_cm3(gt,  spacing), 4),\n",
                "            'inference_s':     round(inference_s, 2),\n",
                "        }\n",
                "        if extra:\n",
                "            row.update(extra)\n",
                "        self._rows.append(row)\n",
                "    def to_dataframe(self):\n",
                "        return pd.DataFrame(self._rows)\n",
                "\n",
                "# â”€â”€ Visualisation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "sns.set_theme(style='darkgrid', palette='muted')\n",
                "MASK_ALPHA = 0.45\n",
                "\n",
                "def _best_slice(mask):\n",
                "    counts = (mask > 0).sum(axis=(0, 1))\n",
                "    idx = int(counts.argmax())\n",
                "    return max(5, min(idx, mask.shape[2] - 6))\n",
                "\n",
                "def plot_model_comparison(mri, predictions, gt=None, slice_idx=None):\n",
                "    cols = ['MRI']\n",
                "    if gt is not None:\n",
                "        cols.append('GT')\n",
                "    cols += list(predictions.keys())\n",
                "    n = len(cols)\n",
                "    if slice_idx is None:\n",
                "        slice_idx = _best_slice(gt) if gt is not None else mri.shape[2] // 2\n",
                "    fig, axes = plt.subplots(1, n, figsize=(n * 3.5, 4))\n",
                "    if n == 1:\n",
                "        axes = [axes]\n",
                "    cmap_list = ['Reds', 'Blues', 'Greens', 'Purples', 'Oranges']\n",
                "    col_idx = 0\n",
                "    axes[col_idx].imshow(mri[:, :, slice_idx].T, cmap='gray', origin='lower', aspect='auto')\n",
                "    axes[col_idx].set_title('MRI (T1c)', fontsize=10)\n",
                "    axes[col_idx].axis('off')\n",
                "    col_idx += 1\n",
                "    if gt is not None:\n",
                "        axes[col_idx].imshow(mri[:, :, slice_idx].T, cmap='gray', origin='lower', aspect='auto')\n",
                "        gm = gt[:, :, slice_idx].T\n",
                "        if gm.any():\n",
                "            axes[col_idx].imshow(np.ma.masked_where(gm == 0, gm), cmap='Greens',\n",
                "                                  alpha=MASK_ALPHA, origin='lower', aspect='auto')\n",
                "        axes[col_idx].set_title('Ground Truth', fontsize=10)\n",
                "        axes[col_idx].axis('off')\n",
                "        col_idx += 1\n",
                "    for (model_name, pred), cmap in zip(predictions.items(), cmap_list):\n",
                "        axes[col_idx].imshow(mri[:, :, slice_idx].T, cmap='gray', origin='lower', aspect='auto')\n",
                "        pm = pred[:, :, slice_idx].T\n",
                "        if pm.any():\n",
                "            axes[col_idx].imshow(np.ma.masked_where(pm == 0, pm), cmap=cmap,\n",
                "                                  alpha=MASK_ALPHA, origin='lower', aspect='auto')\n",
                "        dice = dice_coefficient(pred, gt) if gt is not None else 0.0\n",
                "        axes[col_idx].set_title(f'{model_name}\\nDice={dice:.3f}', fontsize=9, fontweight='bold')\n",
                "        axes[col_idx].axis('off')\n",
                "        col_idx += 1\n",
                "    fig.suptitle(f'SAM Prompt Mode Comparison â€“ Axial slice {slice_idx}', fontsize=13, fontweight='bold')\n",
                "    plt.tight_layout()\n",
                "    return fig\n",
                "\n",
                "print('âœ… Utility functions defined')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ğŸ¤– Cell 6: Load SAM3 â†’ SAM2 Fallback Chain\n",
                "\n",
                "Tries SAM3 first (requires HF access approval), falls back to SAM2 automatically."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# â”€â”€ Cell 6: Load SAM3 â†’ SAM2 â†’ MedSAM2 fallback chain â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "SAM_MODEL_LABEL = None\n",
                "sam_predictor   = None\n",
                "STUB_MODE       = False\n",
                "\n",
                "if DEVICE == 'cpu':\n",
                "    print('âš ï¸  No GPU detected â€“ running in STUB mode')\n",
                "    print('   Go to Runtime â†’ Change runtime type â†’ T4 GPU')\n",
                "    STUB_MODE = True\n",
                "else:\n",
                "    # â”€â”€ Try SAM3 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "    try:\n",
                "        from sam3 import SAM3Predictor\n",
                "        print('â³ Loading SAM3 (facebook/sam3)...')\n",
                "        # NOTE: Use 'facebook/sam3' â€“ 'sam3-hiera-large' variant may not exist yet\n",
                "        sam_predictor   = SAM3Predictor.from_pretrained('facebook/sam3')\n",
                "        sam_predictor.model.to(DEVICE)\n",
                "        SAM_MODEL_LABEL = 'sam3'\n",
                "        print('âœ… SAM3 loaded successfully!')\n",
                "    except ImportError as e:\n",
                "        print(f'SAM3 import failed (not installed): {e}')\n",
                "    except Exception as e:\n",
                "        print(f'SAM3 load failed: {e}')\n",
                "        if 'gated' in str(e).lower() or '401' in str(e) or 'access' in str(e).lower():\n",
                "            print('   â†³ Model is gated â€“ request access at https://huggingface.co/facebook/sam3')\n",
                "\n",
                "    # â”€â”€ Try SAM2 (fallback) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "    if sam_predictor is None:\n",
                "        try:\n",
                "            from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
                "            print('â³ Loading SAM2 (facebook/sam2-hiera-large)...')\n",
                "            sam_predictor   = SAM2ImagePredictor.from_pretrained('facebook/sam2-hiera-large')\n",
                "            sam_predictor.model.to(DEVICE)\n",
                "            SAM_MODEL_LABEL = 'sam2'\n",
                "            print('âœ… SAM2 loaded (SAM3 fallback)')\n",
                "        except Exception as e2:\n",
                "            print(f'SAM2 also failed: {e2}')\n",
                "\n",
                "    # â”€â”€ Try MedSAM2 (second fallback) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "    if sam_predictor is None:\n",
                "        try:\n",
                "            from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
                "            print('â³ Loading MedSAM2 (bowang-lab/MedSAM2)...')\n",
                "            sam_predictor   = SAM2ImagePredictor.from_pretrained('bowang-lab/MedSAM2')\n",
                "            sam_predictor.model.to(DEVICE)\n",
                "            SAM_MODEL_LABEL = 'medsam2'\n",
                "            print('âœ… MedSAM2 loaded (medical-adapted SAM2)')\n",
                "        except Exception as e3:\n",
                "            print(f'MedSAM2 also failed: {e3}')\n",
                "            print('âš ï¸  All models failed â€“ falling back to STUB mode')\n",
                "            STUB_MODE = True\n",
                "\n",
                "print(f'\\nğŸ·ï¸  Model: {SAM_MODEL_LABEL or \"STUB\"} | Stub mode: {STUB_MODE} | Device: {DEVICE}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ğŸ“‚ Cell 7: Load Data Paths"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# â”€â”€ Cell 7: Load data paths and initialise tracker â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "if USE_DRIVE:\n",
                "    brats_paths = get_p01_brats_paths(BRATS_DIR)\n",
                "    mask_paths  = get_p01_mask_paths(MASK_DIR)\n",
                "# else: brats_paths and mask_paths were set in Cell 4B\n",
                "\n",
                "tracker = BenchmarkTracker()\n",
                "\n",
                "print(f'Found {len(brats_paths)} timepoints: {list(brats_paths.keys())}')\n",
                "print(f'Found {len(mask_paths)} masks: {list(mask_paths.keys())}')\n",
                "\n",
                "for tp, mods in brats_paths.items():\n",
                "    print(f'  {tp}: {list(mods.keys())}', 'âœ…' if tp in mask_paths else 'âš ï¸  (no mask)')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ğŸ”§ Cell 8: SAM Inference Helpers"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# â”€â”€ Cell 8: Prompt generation and slice-wise SAM inference â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "from PIL import Image as PILImage\n",
                "\n",
                "def get_tumor_centroid(mask, slice_idx):\n",
                "    \"\"\"Return (x, y) centroid of tumor in axial slice.\"\"\"\n",
                "    s = mask[:, :, slice_idx]\n",
                "    if not s.any():\n",
                "        return (mask.shape[0] // 2, mask.shape[1] // 2)\n",
                "    ys, xs = np.where(s > 0)\n",
                "    return (int(xs.mean()), int(ys.mean()))\n",
                "\n",
                "def get_tumor_bbox(mask, slice_idx):\n",
                "    \"\"\"Return [x0, y0, x1, y1] bounding box of tumor in axial slice.\"\"\"\n",
                "    s = mask[:, :, slice_idx]\n",
                "    if not s.any():\n",
                "        H, W = s.shape\n",
                "        return [W // 4, H // 4, 3*W // 4, 3*H // 4]\n",
                "    ys, xs = np.where(s > 0)\n",
                "    # Add small padding around the GT bbox to simulate realistic prompting\n",
                "    PAD = 3\n",
                "    return [max(0, int(xs.min())-PAD), max(0, int(ys.min())-PAD),\n",
                "            int(xs.max())+PAD, int(ys.max())+PAD]\n",
                "\n",
                "def volume_to_rgb(vol_slice):\n",
                "    \"\"\"Normalise a 2D MRI slice to uint8 RGB.\"\"\"\n",
                "    sl = vol_slice.astype(np.float32)\n",
                "    sl_norm = ((sl - sl.min()) / (sl.max() - sl.min() + 1e-8) * 255).astype(np.uint8)\n",
                "    return np.stack([sl_norm] * 3, axis=-1)   # H x W x 3\n",
                "\n",
                "def slicewise_sam_inference(\n",
                "    predictor,\n",
                "    volume,\n",
                "    gt_mask,\n",
                "    mode='box',         # 'point' | 'box' | 'auto'\n",
                "    n_slices_max=30,\n",
                "    verbose=True,\n",
                "):\n",
                "    \"\"\"\n",
                "    Run SAM2/SAM3 slice-wise on a 3D MRI volume.\n",
                "    Returns a 3D binary mask of the same shape as volume.\n",
                "    \"\"\"\n",
                "    H, W, D = volume.shape\n",
                "    mask_3d = np.zeros((H, W, D), dtype=np.float32)\n",
                "\n",
                "    # Only process slices that contain tumor (guided by GT)\n",
                "    tumor_slices = [s for s in range(D) if (gt_mask[:, :, s] > 0).any()]\n",
                "    slices_to_run = sorted(tumor_slices[:n_slices_max])\n",
                "\n",
                "    if verbose:\n",
                "        print(f'  Running {mode} inference on {len(slices_to_run)} tumor slices...')\n",
                "\n",
                "    for i, s in enumerate(slices_to_run):\n",
                "        rgb = volume_to_rgb(volume[:, :, s])\n",
                "\n",
                "        with torch.inference_mode():\n",
                "            predictor.set_image(rgb)\n",
                "\n",
                "            if mode == 'point':\n",
                "                cx, cy = get_tumor_centroid(gt_mask, s)\n",
                "                masks, scores, _ = predictor.predict(\n",
                "                    point_coords=np.array([[cx, cy]]),\n",
                "                    point_labels=np.array([1]),\n",
                "                    multimask_output=False,\n",
                "                )\n",
                "            elif mode == 'box':\n",
                "                bbox = get_tumor_bbox(gt_mask, s)\n",
                "                masks, scores, _ = predictor.predict(\n",
                "                    box=np.array(bbox)[None, :],\n",
                "                    multimask_output=False,\n",
                "                )\n",
                "            else:  # auto â€“ uses automatic mask generator\n",
                "                try:\n",
                "                    from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator\n",
                "                    amg = SAM2AutomaticMaskGenerator(predictor.model,\n",
                "                                                     points_per_side=16,\n",
                "                                                     pred_iou_thresh=0.85)\n",
                "                    auto_masks = amg.generate(rgb)\n",
                "                    if auto_masks:\n",
                "                        best = max(auto_masks, key=lambda x: x['area'])\n",
                "                        masks = best['segmentation'][None, :, :]\n",
                "                    else:\n",
                "                        masks = np.zeros((1, H, W), dtype=bool)\n",
                "                except Exception as e:\n",
                "                    print(f'    Auto mode error on slice {s}: {e}')\n",
                "                    masks = np.zeros((1, H, W), dtype=bool)\n",
                "\n",
                "        mask_3d[:, :, s] = masks[0].astype(np.float32)\n",
                "\n",
                "        if verbose and (i+1) % 5 == 0:\n",
                "            print(f'    Processed {i+1}/{len(slices_to_run)} slices...')\n",
                "\n",
                "    return mask_3d\n",
                "\n",
                "print('âœ… SAM inference helpers defined')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ğŸš€ Cell 9: Run Inference (All Modes + All Timepoints)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# â”€â”€ Cell 9: Run inference or stub for all modes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "test_timepoints = list(brats_paths.keys())[:2]   # baseline + fu1\n",
                "print(f'Running on timepoints: {test_timepoints}')\n",
                "print(f'Model: {SAM_MODEL_LABEL or \"STUB\"} | Stub: {STUB_MODE}\\n')\n",
                "\n",
                "for tp in test_timepoints:\n",
                "    print(f'â”€â”€â”€ Timepoint: {tp} â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€')\n",
                "    t1c_path = brats_paths[tp].get('t1c')\n",
                "    gt_path  = mask_paths.get(tp)\n",
                "\n",
                "    if not t1c_path or not gt_path:\n",
                "        print(f'  âš ï¸  Missing t1c or mask for {tp}, skipping.')\n",
                "        continue\n",
                "\n",
                "    vol, vol_aff, _  = load_nifti(t1c_path)\n",
                "    gt_arr, _, _     = load_nifti(gt_path)\n",
                "    spacing          = tuple(float(s) for s in nib.load(gt_path).header.get_zooms()[:3])\n",
                "    best_sl          = int(gt_arr.sum(axis=(0, 1)).argmax())\n",
                "\n",
                "    print(f'  Volume shape: {vol.shape}, spacing: {spacing}')\n",
                "    print(f'  GT voxels: {(gt_arr > 0).sum()}, best slice: {best_sl}')\n",
                "\n",
                "    if STUB_MODE or sam_predictor is None:\n",
                "        # â”€â”€ STUB: simulate predictions with morphological ops â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "        gt_bin = (gt_arr > 0)\n",
                "        stub_point = binary_erosion(gt_bin, iterations=1).astype(np.float32)\n",
                "        stub_box   = gt_bin.astype(np.float32)\n",
                "        stub_auto  = binary_erosion(gt_bin, iterations=4).astype(np.float32)\n",
                "\n",
                "        for mode_label, stub_pred in [\n",
                "            ('point_stub', stub_point),\n",
                "            ('box_stub',   stub_box),\n",
                "            ('auto_stub',  stub_auto),\n",
                "        ]:\n",
                "            tracker.add(\n",
                "                model=f'{SAM_MODEL_LABEL or \"sam\"}__{mode_label}',\n",
                "                timepoint=tp,\n",
                "                pred=stub_pred,\n",
                "                gt=gt_arr,\n",
                "                spacing=spacing,\n",
                "                inference_s=0.0,\n",
                "                extra={'is_stub': True},\n",
                "            )\n",
                "            print(f'  [STUB] {mode_label}: dice={dice_coefficient(stub_pred, gt_arr):.3f}')\n",
                "\n",
                "        save_nifti(stub_box, vol_aff, OUT_DIR / f'sam_stub_{tp}_box_pred.nii.gz')\n",
                "\n",
                "    else:\n",
                "        # â”€â”€ REAL INFERENCE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "        for mode in ['point', 'box']:\n",
                "            print(f'\\n  â–¶ Mode: {mode}')\n",
                "            with Timer(f'{SAM_MODEL_LABEL} {mode} {tp}') as t:\n",
                "                pred = slicewise_sam_inference(\n",
                "                    sam_predictor, vol, gt_arr,\n",
                "                    mode=mode, n_slices_max=30, verbose=True,\n",
                "                )\n",
                "            dice = dice_coefficient(pred, gt_arr)\n",
                "            iou  = iou_score(pred, gt_arr)\n",
                "            print(f'  Dice={dice:.4f} | IoU={iou:.4f} | Time={t.elapsed:.1f}s')\n",
                "\n",
                "            tracker.add(\n",
                "                model=f'{SAM_MODEL_LABEL}__{mode}',\n",
                "                timepoint=tp,\n",
                "                pred=pred,\n",
                "                gt=gt_arr,\n",
                "                spacing=spacing,\n",
                "                inference_s=t.elapsed,\n",
                "            )\n",
                "            save_nifti(pred, vol_aff,\n",
                "                       OUT_DIR / f'{SAM_MODEL_LABEL}_{tp}_{mode}_pred.nii.gz')\n",
                "\n",
                "print('\\nâœ… Inference complete!')\n",
                "df = tracker.to_dataframe()\n",
                "print(df[['model', 'timepoint', 'dice', 'iou', 'volume_pred_cm3', 'inference_s']].to_string(index=False))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ğŸ“Š Cell 10: Results Visualisation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# â”€â”€ Cell 10: Visualise comparison of prompt modes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "tp = 'baseline'\n",
                "t1c_arr, _, _ = load_nifti(brats_paths[tp]['t1c'])\n",
                "gt_arr,  _, _ = load_nifti(mask_paths[tp])\n",
                "sam_label = SAM_MODEL_LABEL or 'sam'\n",
                "\n",
                "# Collect prediction masks\n",
                "preds = {}\n",
                "\n",
                "if STUB_MODE or sam_predictor is None:\n",
                "    # Load stub predictions\n",
                "    for mode in ['point_stub', 'box_stub', 'auto_stub']:\n",
                "        p = OUT_DIR / f'sam_stub_{tp}_box_pred.nii.gz'\n",
                "        if p.exists():\n",
                "            arr, _, _ = load_nifti(str(p))\n",
                "            preds[f'stub_box'] = arr\n",
                "            break\n",
                "    # Also generate stubs on-the-fly for comparison\n",
                "    gt_bin = (gt_arr > 0)\n",
                "    preds['point_stub'] = binary_erosion(gt_bin, iterations=1).astype(np.float32)\n",
                "    preds['box_stub']   = gt_bin.astype(np.float32)\n",
                "    preds['auto_stub']  = binary_erosion(gt_bin, iterations=4).astype(np.float32)\n",
                "else:\n",
                "    for mode in ['point', 'box']:\n",
                "        p = OUT_DIR / f'{sam_label}_{tp}_{mode}_pred.nii.gz'\n",
                "        if p.exists():\n",
                "            arr, _, _ = load_nifti(str(p))\n",
                "            preds[f'{sam_label}_{mode}'] = arr\n",
                "\n",
                "if preds:\n",
                "    fig = plot_model_comparison(mri=t1c_arr, predictions=preds, gt=gt_arr)\n",
                "    plt.savefig(OUT_DIR / f'{sam_label}_modes_comparison.png', dpi=120, bbox_inches='tight')\n",
                "    plt.show()\n",
                "    print(f'âœ… Saved: {OUT_DIR}/{sam_label}_modes_comparison.png')\n",
                "else:\n",
                "    print('âš ï¸  No prediction files found to visualise.')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# â”€â”€ Cell 10B: Metrics bar chart â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "df = tracker.to_dataframe()\n",
                "\n",
                "if not df.empty and 'dice' in df.columns:\n",
                "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "    # Dice\n",
                "    df_sorted = df.sort_values('dice', ascending=False)\n",
                "    palette = sns.color_palette('viridis', n_colors=len(df_sorted))\n",
                "    bars = axes[0].bar(df_sorted['model'] + '\\n' + df_sorted['timepoint'],\n",
                "                       df_sorted['dice'], color=palette, edgecolor='white')\n",
                "    axes[0].bar_label(bars, fmt='%.3f', padding=3, fontsize=9)\n",
                "    axes[0].set_title('Dice Score by Model/Mode', fontsize=13, fontweight='bold')\n",
                "    axes[0].set_ylim(0, 1.15)\n",
                "    axes[0].set_ylabel('Dice')\n",
                "    axes[0].tick_params(axis='x', rotation=45)\n",
                "    axes[0].spines[['top', 'right']].set_visible(False)\n",
                "\n",
                "    # IoU\n",
                "    df_sorted2 = df.sort_values('iou', ascending=False)\n",
                "    palette2 = sns.color_palette('plasma', n_colors=len(df_sorted2))\n",
                "    bars2 = axes[1].bar(df_sorted2['model'] + '\\n' + df_sorted2['timepoint'],\n",
                "                        df_sorted2['iou'], color=palette2, edgecolor='white')\n",
                "    axes[1].bar_label(bars2, fmt='%.3f', padding=3, fontsize=9)\n",
                "    axes[1].set_title('IoU Score by Model/Mode', fontsize=13, fontweight='bold')\n",
                "    axes[1].set_ylim(0, 1.15)\n",
                "    axes[1].set_ylabel('IoU')\n",
                "    axes[1].tick_params(axis='x', rotation=45)\n",
                "    axes[1].spines[['top', 'right']].set_visible(False)\n",
                "\n",
                "    plt.suptitle(f'SAM Model Benchmark â€“ {sam_label}', fontsize=15, fontweight='bold', y=1.02)\n",
                "    plt.tight_layout()\n",
                "    plt.savefig(OUT_DIR / f'{sam_label}_metrics_chart.png', dpi=120, bbox_inches='tight')\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ğŸ’¾ Cell 11: Save Results & Download"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# â”€â”€ Cell 11: Save CSV results and download to local machine â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "import zipfile\n",
                "\n",
                "df = tracker.to_dataframe()\n",
                "csv_path = OUT_DIR / f'{sam_label}_metrics.csv'\n",
                "df.to_csv(csv_path, index=False)\n",
                "print(f'âœ… Saved metrics: {csv_path}')\n",
                "print(df[['model', 'timepoint', 'dice', 'iou', 'volume_pred_cm3', 'inference_s']].to_string(index=False))\n",
                "\n",
                "# Zip outputs for download\n",
                "zip_path = f'/content/{sam_label}_outputs.zip'\n",
                "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
                "    for f in OUT_DIR.glob('*'):\n",
                "        zf.write(f, f.name)\n",
                "print(f'\\nğŸ“¦ Outputs zipped: {zip_path}')\n",
                "\n",
                "# Download the zip (triggers browser download in Colab)\n",
                "try:\n",
                "    from google.colab import files\n",
                "    files.download(zip_path)\n",
                "    print('â¬‡ï¸  Download triggered!')\n",
                "except Exception:\n",
                "    print(f'  (Not in Colab â€“ find zip at: {zip_path})')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ğŸ“‹ Cell 12: Notes & Findings\n",
                "\n",
                "Fill in after running inference. Copy these results back to your local notebook.\n",
                "\n",
                "| Item | Finding |\n",
                "|------|----------|\n",
                "| GPU used | _(e.g. T4 16GB)_ |\n",
                "| Model loaded | _(SAM3 / SAM2 / MedSAM2)_ |\n",
                "| Best prompt mode | _(point / box / auto)_ |\n",
                "| Dice (box prompt, baseline) | _fill in_ |\n",
                "| Dice (point prompt, baseline) | _fill in_ |\n",
                "| Inference time per 30-slice volume | _fill in_ |\n",
                "| GPU VRAM used | _fill in (check nvidia-smi below)_ |\n",
                "| **Key finding** | |\n",
                "| **Recommendation** | Box-prompted SAM2/3 for ensemble; interactive mode for clinical UI |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# â”€â”€ Cell 12B: GPU memory usage report â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "if DEVICE == 'cuda':\n",
                "    allocated = torch.cuda.memory_allocated() / 1e9\n",
                "    reserved  = torch.cuda.memory_reserved() / 1e9\n",
                "    total     = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
                "    print(f'GPU Memory:')\n",
                "    print(f'  Allocated : {allocated:.2f} GB')\n",
                "    print(f'  Reserved  : {reserved:.2f} GB')\n",
                "    print(f'  Total VRAM: {total:.2f} GB')\n",
                "\n",
                "    import subprocess\n",
                "    result = subprocess.run('nvidia-smi', capture_output=True, text=True)\n",
                "    print('\\nnvidia-smi output:')\n",
                "    print(result.stdout)\n",
                "else:\n",
                "    print('CPU mode â€“ no GPU memory info')"
            ]
        }
    ]
}